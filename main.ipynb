{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame(gym.Env):\n",
    "    # other metadata avail, render.moldes unncessary if render() is not implemented\n",
    "    metadata = {'render.modes' : ['console', 'rgb_array']}\n",
    "\n",
    "    n_actions = 3\n",
    "\n",
    "    # actions\n",
    "    LEFT = 0\n",
    "    STRAIGHT = 1\n",
    "    RIGHT = 2\n",
    "\n",
    "    # states\n",
    "    EMPTY = 0\n",
    "    SNAKE = 1\n",
    "    WALL = 2\n",
    "    FOOD = 3\n",
    "\n",
    "    REWARD_WALL_HIT = -20\n",
    "    REWARD_PER_STEP_TOWARDS_FOOD = 1 # avoid hitting walls on purpose\n",
    "    REWARD_PER_FOOD = 50\n",
    "    MAX_STEPS_AFTER_FOOD = 200 # avoid loop\n",
    "\n",
    "\n",
    "    def grid_distance(self, pos1, pos2):\n",
    "        # calculate euclidean distance between 2 points\n",
    "        return np.linalg.norm(np.array(pos1, dtype=np.float32) - np.array(pos2, dtype=np.float32))\n",
    "    \n",
    "    \n",
    "    def __init__(self, grid_size=10):\n",
    "        super(SnakeGame, self).__init__()\n",
    "\n",
    "        # steps init\n",
    "        self.stepnum = 0\n",
    "        self.last_food_step = 0\n",
    "\n",
    "        # grid init\n",
    "        self.grid_size = grid_size\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.uint8) + self.EMPTY # EMPTY is zero so it doesn't matter (in case its not)\n",
    "        \n",
    "        # wall init\n",
    "        self.grid[0, :] = self.WALL # UP\n",
    "        self.grid[:, 0] = self.WALL # LEFT\n",
    "        self.grid[self.grid_size - 1, :] = self.WALL # DOWN\n",
    "        self.grid[:, self.grid_size - 1] = self.WALL # RIGHT\n",
    "\n",
    "        # snake init\n",
    "        self.snake_coord = [(1, 1), (2, 1)] # top left\n",
    "\n",
    "        for coord in self.snake_coord:\n",
    "            self.grid[coord] = self.SNAKE\n",
    "\n",
    "        # food init\n",
    "        self.grid[3, 3] = self.FOOD\n",
    "\n",
    "        # distance calculation\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        # save init setup\n",
    "        self.init_grid = self.grid.copy()\n",
    "        self.init_snake_coord = self.snake_coord.copy()\n",
    "\n",
    "        # action space\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "\n",
    "        # observation(state) space\n",
    "        self.observation_space = spaces.Dict(\n",
    "            spaces={\n",
    "                \"position\" : spaces.Box(low=0, high=(self.grid_size - 1), shape=(2,), dtype=np.int32),\n",
    "                \"direction\" : spaces.Box(low=-1, high=1, shape=(2,), dtype=np.int32),\n",
    "                \"grid\" : spaces.Box(low=0, high=3, shape=(self.grid_size, self.grid_size), dtype=np.uint8)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        # to init position\n",
    "        self.stepnum = 0\n",
    "        self.last_food_step = 0\n",
    "        self.grid = self.init_grid.copy()\n",
    "        self.snake_coord = self.init_snake_coord.copy()\n",
    "\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        obs = self._get_obs() # state space\n",
    "        info = {}\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        position = np.array(self.snake_coord[-1], dtype=np.int32)\n",
    "        direction = (np.array(self.snake_coord[-1]) - np.array(self.snake_coord[-2])).astype(np.int32)\n",
    "        grid = self.grid \n",
    "\n",
    "        obs = {\n",
    "            \"position\" : position,\n",
    "            \"direction\" : direction,\n",
    "            \"grid\" : grid\n",
    "        }\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        direction = np.array(self.snake_coord[-1]) - np.array(self.snake_coord[-2])\n",
    "\n",
    "        if action == self.STRAIGHT:\n",
    "            step = direction # towards the direction the snake faces\n",
    "        elif action == self.RIGHT:\n",
    "            # rotation matrix\n",
    "            step = np.array( [direction[1], -direction[0]] )\n",
    "        elif action == self.LEFT:\n",
    "            step = np.array( [-direction[1], direction[0]] )\n",
    "        \n",
    "        # new head\n",
    "        new_coord = (np.array(self.snake_coord[-1]) + step).astype(np.int32)\n",
    "        self.snake_coord.append( (new_coord[0], new_coord[1]) )\n",
    "\n",
    "        new_pos = self.snake_coord[-1]\n",
    "        new_pos_type = self.grid[new_pos]\n",
    "        self.grid[new_pos] = self.SNAKE\n",
    "\n",
    "        done = False\n",
    "        reward = 0 # calculated later\n",
    "\n",
    "        if new_pos_type == self.FOOD:\n",
    "            reward += self.REWARD_PER_FOOD\n",
    "            self.last_food_step = self.stepnum\n",
    "\n",
    "            # new food\n",
    "            empty_tiles = np.argwhere(self.grid == self.EMPTY)\n",
    "\n",
    "            if len(empty_tiles):\n",
    "                new_food_pos = empty_tiles[np.random.randint(0, len(empty_tiles))]\n",
    "                self.grid[new_food_pos[0], new_food_pos[1]] = self.FOOD\n",
    "            else:\n",
    "                done = True\n",
    "        else:\n",
    "            self.grid[self.snake_coord[0]] = self.EMPTY # empty the snake tail\n",
    "            self.snake_coord = self.snake_coord[1:]\n",
    "\n",
    "            if (new_pos_type == self.WALL) or (new_pos_type == self.SNAKE):\n",
    "                done = True\n",
    "                reward += self.REWARD_WALL_HIT\n",
    "        \n",
    "        head_dist_to_food_prev = self.head_dist_to_food\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        # reward for distance between snake <-> food\n",
    "        if head_dist_to_food_prev > self.head_dist_to_food:\n",
    "            reward += self.REWARD_PER_STEP_TOWARDS_FOOD\n",
    "        elif head_dist_to_food_prev < self.head_dist_to_food:\n",
    "            reward -= self.REWARD_PER_STEP_TOWARDS_FOOD\n",
    "        \n",
    "        # max steps since no food\n",
    "        if ((self.stepnum - self.last_food_step) > self.MAX_STEPS_AFTER_FOOD):\n",
    "            done = True\n",
    "        \n",
    "        self.stepnum += 1\n",
    "\n",
    "        # return observation, reward, done, truncated, info\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "\n",
    "    def snake_plot(self, plot_inline=False):\n",
    "        wall_idx = (self.grid == self.WALL)\n",
    "        snake_idx = (self.grid == self.SNAKE)\n",
    "        food_idx = (self.grid == self.FOOD)\n",
    "\n",
    "        # colour array for plot\n",
    "        colour_arr = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.uint8) + 255 # default to white\n",
    "        colour_arr[wall_idx, :] = np.array([0, 0, 0])\n",
    "        colour_arr[snake_idx, :] = np.array([255, 196, 0])\n",
    "        colour_arr[food_idx, :] = np.array([30, 47, 135])\n",
    "\n",
    "        return colour_arr\n",
    "    \n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        if mode == 'console':\n",
    "            print(self.grid)\n",
    "        elif mode == 'rgb_array':\n",
    "            return self.snake_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env check\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = SnakeGame()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os\n",
    "\n",
    "# log\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# env\n",
    "env = SnakeGame()\n",
    "\n",
    "# wrap env with monitor\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# cb fn -> periodically evaluate the model and save the best version\n",
    "eval_cb = EvalCallback(env, best_model_save_path='./best_model',\n",
    "                       log_path='./log',\n",
    "                       eval_freq=5000,\n",
    "                       deterministic=False,\n",
    "                       render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2005000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2010000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2015000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2020000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2025000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2030000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2035000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2045000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2050000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2055000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2060000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2065000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2070000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2075000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2085000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2090000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2095000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2100000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2105000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2110000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2115000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2125000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2130000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2135000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2140000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2145000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2150000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2155000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2165000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2170000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2175000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2180000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2185000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2190000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2195000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2205000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2210000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2215000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2220000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2225000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2230000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2235000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2245000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2250000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2255000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2260000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2265000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2270000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2275000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2285000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2290000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2295000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2300000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2305000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2310000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2315000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2325000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2330000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2335000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2340000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2345000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2350000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2355000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2365000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2370000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2375000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2380000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2385000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2390000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2395000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2400000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2405000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2410000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2415000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2420000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2425000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2430000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2435000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2445000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2450000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2455000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2460000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2465000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2470000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2475000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2480000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2485000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2490000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2495000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2500000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2505000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2510000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2515000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2520000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2525000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2530000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2535000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2540000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2545000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2550000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2555000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2560000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2565000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2570000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2575000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2580000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2585000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2590000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2595000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2600000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2605000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2610000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2615000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2620000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2625000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2630000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2635000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2640000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2645000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2650000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2655000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2660000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2665000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2670000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2675000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2680000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2685000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2690000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2695000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2700000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2705000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2710000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2715000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2720000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2725000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2730000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2735000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2740000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2745000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2750000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2755000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2760000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2765000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2770000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2775000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2780000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2785000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2790000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2795000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2800000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2805000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2810000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2815000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2820000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2825000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2830000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2835000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2840000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2845000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2850000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2855000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2860000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2865000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2870000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2875000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2880000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2885000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2890000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2895000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2900000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2905000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2910000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2915000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2920000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2925000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2930000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2935000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2940000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2945000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2950000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2955000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2960000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2965000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2970000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2975000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2980000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2985000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2990000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=2995000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3000000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3005000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3010000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3015000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3020000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3025000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3030000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3035000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3040000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3045000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3050000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3055000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3060000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3065000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3070000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3075000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3080000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3085000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3090000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3095000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3100000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3105000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3110000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3115000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3120000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3125000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3130000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3135000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3140000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3145000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3150000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3155000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3160000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3165000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3170000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3175000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3180000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3185000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3190000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3195000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3200000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3205000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3210000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3215000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3220000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3225000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3230000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3235000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3240000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3245000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3250000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3255000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3260000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3265000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3270000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3275000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3280000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3285000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3290000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3295000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3300000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3305000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3310000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3315000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3320000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3325000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3330000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3335000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3340000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3345000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3350000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3355000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3360000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3365000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3370000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3375000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3380000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3385000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3390000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3395000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3400000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3405000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3410000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3415000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3420000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3425000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3430000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3435000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3440000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3445000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3450000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3455000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3460000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3465000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3470000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3475000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3480000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3485000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3490000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3495000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3500000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3505000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3510000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3515000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3520000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3525000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3530000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3535000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3540000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3545000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3550000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3555000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3560000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3565000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3570000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3575000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3580000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3585000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3590000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3595000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3600000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3605000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3610000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3615000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3620000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3625000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3630000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3635000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3640000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3645000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3650000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3655000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3660000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3665000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3670000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3675000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3680000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3685000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3690000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3695000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3700000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3705000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3710000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3715000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3720000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3725000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3730000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3735000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3740000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3745000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3750000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3755000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3760000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3765000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3770000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3775000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3780000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3785000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3790000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3795000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3800000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3805000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3810000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3815000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3820000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3825000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3830000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3835000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3840000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3845000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3850000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3855000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3860000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3865000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3870000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3875000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3880000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3885000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3890000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3895000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3900000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3905000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3910000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3915000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3920000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3925000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3930000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3935000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3940000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3945000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3950000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3955000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3960000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3965000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3970000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3975000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3980000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3985000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3990000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=3995000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4000000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4005000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4010000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4015000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4020000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4025000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4030000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4035000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4040000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4045000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4050000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4055000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4060000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4065000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4070000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4075000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4080000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4085000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4090000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4095000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4100000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4105000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4110000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4115000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4120000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4125000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4130000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4135000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4140000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4145000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4150000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4155000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4160000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4165000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4170000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4175000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4180000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4185000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4190000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4195000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4200000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4205000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4210000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4215000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4220000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4225000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4230000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4235000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4240000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4245000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4250000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4255000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4260000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4265000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4270000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4275000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4280000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4285000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4290000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4295000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4300000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4305000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4310000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4315000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4320000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4325000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4330000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4335000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4340000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4345000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4350000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4355000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4360000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4365000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4370000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4375000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4380000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4385000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4390000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4395000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4400000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4405000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4410000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4415000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4420000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4425000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4430000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4435000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4440000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4445000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4450000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4455000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4460000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4465000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4470000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4475000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4480000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4485000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4490000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4495000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4500000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4505000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4510000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4515000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4520000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4525000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4530000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4535000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4540000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4545000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4550000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4555000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4560000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4565000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4570000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4575000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4580000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4585000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4590000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4595000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4600000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4605000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4610000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4615000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4620000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4625000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4630000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4635000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4640000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4645000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4650000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4655000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4660000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4665000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4670000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4675000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4680000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4685000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4690000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=4695000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "# Proximal Policy Optimization\n",
    "\n",
    "# PPO hyperparam\n",
    "PPO_model_args = {\n",
    "    \"learning_rate\" : 0.03,\n",
    "    \"gamma\" : 0.6, # discount factor for further rewards [0, 1]\n",
    "    \"verbose\" : 0, # 1 -> more info on training steps\n",
    "    \"seed\" : 523,\n",
    "    \"ent_coef\" : 0.2, # entropy coef -> encourage exploration\n",
    "    \"clip_range\" : 0.2 # limits p of action difference\n",
    "}\n",
    "\n",
    "# Multi Input Policy since we have 1+ states as an 'input'\n",
    "model = PPO('MultiInputPolicy', env, **PPO_model_args)\n",
    "\n",
    "if os.path.exists(\"best_model/best_model.zip\"):\n",
    "    model.set_parameters(\"best_model/best_model.zip\")\n",
    "\n",
    "model.learn(6000000, callback=eval_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "\n",
    "# test\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# for img gif\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "frames = []\n",
    "fps = 18\n",
    "\n",
    "n_steps = 1000000\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # preprocess the obs to match the model's input format\n",
    "    action, _ = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(f\"Step {step + 1} \\nAction: {action} \\nTotal Reward: {total_reward}\")\n",
    "\n",
    "    if done:\n",
    "        print(f\"Game Over! Total Reward: {total_reward}\")\n",
    "        break\n",
    "\n",
    "fig.subplots.adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
    "\n",
    "anim = animation.ArtistAnimation(fig, frames, interval=int(1000/fps), blit=True, repeat_delay=1000)\n",
    "anim.save(\"snake_game.gif\", dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
