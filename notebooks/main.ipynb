{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame(gym.Env):\n",
    "    # other metadata avail, render.moldes unncessary if render() is not implemented\n",
    "    metadata = {'render.modes' : ['console', 'rgb_array']}\n",
    "\n",
    "    n_actions = 3\n",
    "\n",
    "    # actions\n",
    "    LEFT = 0\n",
    "    STRAIGHT = 1\n",
    "    RIGHT = 2\n",
    "\n",
    "    # states\n",
    "    EMPTY = 0\n",
    "    SNAKE = 1\n",
    "    WALL = 2\n",
    "    FOOD = 3\n",
    "\n",
    "    REWARD_WALL_HIT = -50\n",
    "    REWARD_PER_STEP_TOWARDS_FOOD = 5 # avoid hitting walls on purpose\n",
    "    REWARD_PER_FOOD = 100\n",
    "    MAX_STEPS_AFTER_FOOD = 200 # avoid loop\n",
    "\n",
    "\n",
    "    def grid_distance(self, pos1, pos2):\n",
    "        # calculate euclidean distance between 2 points\n",
    "        return np.linalg.norm(np.array(pos1, dtype=np.float32) - np.array(pos2, dtype=np.float32))\n",
    "\n",
    "    \n",
    "    def __init__(self, grid_size=20):\n",
    "        super(SnakeGame, self).__init__()\n",
    "\n",
    "        # steps init\n",
    "        self.stepnum = 0\n",
    "        self.last_food_step = 0\n",
    "\n",
    "        # grid init\n",
    "        self.grid_size = grid_size\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.uint8) + self.EMPTY # EMPTY is zero so it doesn't matter (in case its not)\n",
    "        \n",
    "        # wall init\n",
    "        self.grid[0, :] = self.WALL # UP\n",
    "        self.grid[:, 0] = self.WALL # LEFT\n",
    "        self.grid[self.grid_size - 1, :] = self.WALL # DOWN\n",
    "        self.grid[:, self.grid_size - 1] = self.WALL # RIGHT\n",
    "\n",
    "        # snake init\n",
    "        # self.snake_coordinates = [ (1,1), (2,1) ]\n",
    "        self.snake_coord = [(4, 3), (4, 4)] # top left\n",
    "\n",
    "        for coord in self.snake_coord:\n",
    "            self.grid[coord] = self.SNAKE\n",
    "\n",
    "        # food init\n",
    "        self.grid[3, 3] = self.FOOD\n",
    "\n",
    "        # distance calculation\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        # save init setup\n",
    "        self.init_grid = self.grid.copy()\n",
    "        self.init_snake_coord = self.snake_coord.copy()\n",
    "\n",
    "        # action space\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "\n",
    "        # observation(state) space\n",
    "        self.observation_space = spaces.Dict(\n",
    "            spaces={\n",
    "                \"position\" : spaces.Box(low=0, high=(self.grid_size - 1), shape=(2,), dtype=np.int32),\n",
    "                \"direction\" : spaces.Box(low=-1, high=1, shape=(2,), dtype=np.int32),\n",
    "                \"grid\" : spaces.Box(low=0, high=3, shape=(self.grid_size * self.grid_size,), dtype=np.uint8)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        import random\n",
    "        # to init position\n",
    "        self.stepnum = 0\n",
    "        self.last_food_step = 0\n",
    "        self.grid = self.init_grid.copy()\n",
    "        self.snake_coord = self.init_snake_coord.copy()\n",
    "\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        obs = self._get_obs() # state space\n",
    "        info = {}\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        position = np.array(self.snake_coord[-1], dtype=np.int32)\n",
    "        direction = (np.array(self.snake_coord[-1]) - np.array(self.snake_coord[-2])).astype(np.int32)\n",
    "        grid = self.grid.flatten()\n",
    "\n",
    "        obs = {\n",
    "            \"position\" : position,\n",
    "            \"direction\" : direction,\n",
    "            \"grid\" : grid\n",
    "        }\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        direction = np.array(self.snake_coord[-1]) - np.array(self.snake_coord[-2])\n",
    "\n",
    "        if action == self.STRAIGHT:\n",
    "            step = direction # towards the direction the snake faces\n",
    "        elif action == self.RIGHT:\n",
    "            # rotation matrix\n",
    "            step = np.array( [direction[1], -direction[0]] )\n",
    "        elif action == self.LEFT:\n",
    "            step = np.array( [-direction[1], direction[0]] )\n",
    "        \n",
    "        # new head\n",
    "        new_coord = (np.array(self.snake_coord[-1]) + step).astype(np.int32)\n",
    "\n",
    "        if not (0 <= new_coord[0] < self.grid_size and 0 <= new_coord[1] < self.grid_size):\n",
    "            return self._get_obs(), self.REWARD_WALL_HIT, True, False, {}\n",
    "\n",
    "        self.snake_coord.append( (new_coord[0], new_coord[1]) )\n",
    "\n",
    "        new_pos = self.snake_coord[-1]\n",
    "        new_pos_type = self.grid[new_pos]\n",
    "        self.grid[new_pos] = self.SNAKE\n",
    "\n",
    "        done = False\n",
    "        reward = 0 # calculated later\n",
    "\n",
    "        if new_pos_type == self.FOOD:\n",
    "            reward += self.REWARD_PER_FOOD\n",
    "            self.last_food_step = self.stepnum\n",
    "\n",
    "            # new food\n",
    "            empty_tiles = np.argwhere(self.grid == self.EMPTY)\n",
    "\n",
    "            if len(empty_tiles):\n",
    "                new_food_pos = empty_tiles[np.random.randint(0, len(empty_tiles))]\n",
    "                self.grid[new_food_pos[0], new_food_pos[1]] = self.FOOD\n",
    "            else:\n",
    "                done = True\n",
    "            \n",
    "        else:\n",
    "            self.grid[self.snake_coord[0]] = self.EMPTY # empty the snake tail\n",
    "            self.snake_coord = self.snake_coord[1:]\n",
    "\n",
    "            if (new_pos_type == self.WALL) or (new_pos_type == self.SNAKE):\n",
    "                done = True\n",
    "                reward += self.REWARD_WALL_HIT\n",
    "        \n",
    "        head_dist_to_food_prev = self.head_dist_to_food\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        # reward for distance between snake <-> food\n",
    "        if head_dist_to_food_prev > self.head_dist_to_food:\n",
    "            reward += self.REWARD_PER_STEP_TOWARDS_FOOD\n",
    "        elif head_dist_to_food_prev < self.head_dist_to_food:\n",
    "            reward -= self.REWARD_PER_STEP_TOWARDS_FOOD * 2\n",
    "        \n",
    "        # max steps since no food\n",
    "        if ((self.stepnum - self.last_food_step) > self.MAX_STEPS_AFTER_FOOD):\n",
    "            done = True\n",
    "        \n",
    "        self.stepnum += 1\n",
    "\n",
    "        # print(f\"Step: {self.stepnum}, Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "        # print(f\"Snake Head: {self.snake_coord[-1]}, Distance to Food: {self.head_dist_to_food}\")\n",
    "        # print(f\"New Position Type: {new_pos_type}\")\n",
    "\n",
    "        # return observation, reward, done, truncated, info\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "\n",
    "    def snake_plot(self, plot_inline=False):\n",
    "        wall_idx = (self.grid == self.WALL)\n",
    "        snake_idx = (self.grid == self.SNAKE)\n",
    "        food_idx = (self.grid == self.FOOD)\n",
    "\n",
    "        # colour array for plot\n",
    "        colour_arr = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.uint8) + 255 # default to white\n",
    "        colour_arr[wall_idx, :] = np.array([0, 0, 0])\n",
    "        colour_arr[snake_idx, :] = np.array([255, 196, 0])\n",
    "        colour_arr[food_idx, :] = np.array([30, 47, 135])\n",
    "\n",
    "        return colour_arr\n",
    "    \n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        if mode == 'console':\n",
    "            print(self.grid)\n",
    "        elif mode == 'rgb_array':\n",
    "            return self.snake_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env check\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = SnakeGame()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os\n",
    "\n",
    "# log\n",
    "log_dir = \"../log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# env\n",
    "env = SnakeGame()\n",
    "\n",
    "# wrap env with monitor\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# cb fn -> periodically evaluate the model and save the best version\n",
    "eval_cb = EvalCallback(env, best_model_save_path='../model',\n",
    "                       log_path='../log',\n",
    "                       eval_freq=5000,\n",
    "                       deterministic=False,\n",
    "                       render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m\"\u001b[39m\u001b[33m../model/best_model.zip\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     18\u001b[39m     model.set_parameters(\u001b[33m\"\u001b[39m\u001b[33m../model/best_model.zip\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m6000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_cb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# model.learn(160000, callback=eval_cb)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28mself\u001b[39m._dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m callback.on_training_end()\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:278\u001b[39m, in \u001b[36mPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[32m    277\u001b[39m     th.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.policy.parameters(), \u001b[38;5;28mself\u001b[39m.max_grad_norm)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28mself\u001b[39m._n_updates += \u001b[32m1\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/RL-snake-game/.venv/lib/python3.12/site-packages/torch/optim/adam.py:476\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    474\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m         denom = (\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / bias_correction2_sqrt).add_(eps)\n\u001b[32m    478\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "# Proximal Policy Optimization\n",
    "\n",
    "# PPO hyperparam\n",
    "PPO_model_args = {\n",
    "    \"learning_rate\" : 0.002,\n",
    "    \"gamma\" : 0.99, # discount factor for further rewards [0, 1]\n",
    "    \"verbose\" : 0, # 1 -> more info on training steps\n",
    "    \"seed\" : 523,\n",
    "    \"ent_coef\" : 0.2, # entropy coef -> encourage exploration\n",
    "    \"clip_range\" : 0.2 # limits p of action difference\n",
    "}\n",
    "\n",
    "# Multi Input Policy since we have 1+ states as an 'input'\n",
    "model = PPO('MultiInputPolicy', env, **PPO_model_args)\n",
    "\n",
    "if os.path.exists(\"../model/best_model.zip\"):\n",
    "    model.set_parameters(\"../model/best_model.zip\")\n",
    "\n",
    "model.learn(6000000, callback=eval_cb)\n",
    "# model.learn(160000, callback=eval_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 \n",
      "Action: 0 \n",
      "Total Reward: 5\n",
      "Step 2 \n",
      "Action: 0 \n",
      "Total Reward: 95\n",
      "Step 3 \n",
      "Action: 2 \n",
      "Total Reward: 85\n",
      "Step 4 \n",
      "Action: 1 \n",
      "Total Reward: 75\n",
      "Step 5 \n",
      "Action: 2 \n",
      "Total Reward: 80\n",
      "Step 6 \n",
      "Action: 1 \n",
      "Total Reward: 85\n",
      "Step 7 \n",
      "Action: 2 \n",
      "Total Reward: 90\n",
      "Step 8 \n",
      "Action: 1 \n",
      "Total Reward: 95\n",
      "Step 9 \n",
      "Action: 1 \n",
      "Total Reward: 100\n",
      "Step 10 \n",
      "Action: 1 \n",
      "Total Reward: 90\n",
      "Step 11 \n",
      "Action: 1 \n",
      "Total Reward: 80\n",
      "Step 12 \n",
      "Action: 1 \n",
      "Total Reward: 70\n",
      "Step 13 \n",
      "Action: 2 \n",
      "Total Reward: 60\n",
      "Step 14 \n",
      "Action: 1 \n",
      "Total Reward: 50\n",
      "Step 15 \n",
      "Action: 0 \n",
      "Total Reward: 40\n",
      "Step 16 \n",
      "Action: 0 \n",
      "Total Reward: 45\n",
      "Step 17 \n",
      "Action: 2 \n",
      "Total Reward: 35\n",
      "Step 18 \n",
      "Action: 2 \n",
      "Total Reward: 25\n",
      "Step 19 \n",
      "Action: 2 \n",
      "Total Reward: 30\n",
      "Step 20 \n",
      "Action: 1 \n",
      "Total Reward: 35\n",
      "Step 21 \n",
      "Action: 0 \n",
      "Total Reward: 25\n",
      "Step 22 \n",
      "Action: 1 \n",
      "Total Reward: 15\n",
      "Step 23 \n",
      "Action: 1 \n",
      "Total Reward: -45\n",
      "Game Over! Total Reward: -45\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "\n",
    "# test\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# for img gif\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "frames = []\n",
    "fps = 18\n",
    "\n",
    "n_steps = 1000000\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # preprocess the obs to match the model's input format\n",
    "    action, _ = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(f\"Step {step + 1} \\nAction: {action} \\nTotal Reward: {total_reward}\")\n",
    "    \n",
    "    frames.append([ax.imshow(env.unwrapped.render(mode='rgb_array'), animated=True)])\n",
    "\n",
    "    if done:\n",
    "        print(f\"Game Over! Total Reward: {total_reward}\")\n",
    "        break\n",
    "\n",
    "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
    "\n",
    "anim = animation.ArtistAnimation(fig, frames, interval=int(1000/fps), blit=True, repeat_delay=1000)\n",
    "anim.save(\"../snake_game.gif\", dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
