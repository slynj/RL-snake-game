{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame(gym.Env):\n",
    "    # other metadata avail, render.moldes unncessary if render() is not implemented\n",
    "    metadata = {'render.modes' : ['console', 'rgb_array']}\n",
    "\n",
    "    n_actions = 3\n",
    "\n",
    "    # actions\n",
    "    LEFT = 0\n",
    "    STRAIGHT = 1\n",
    "    RIGHT = 2\n",
    "\n",
    "    # states\n",
    "    EMPTY = 0\n",
    "    SNAKE = 1\n",
    "    WALL = 2\n",
    "    FOOD = 3\n",
    "\n",
    "    REWARD_WALL_HIT = -50\n",
    "    REWARD_PER_STEP_TOWARDS_FOOD = 5 # avoid hitting walls on purpose\n",
    "    REWARD_PER_FOOD = 100\n",
    "    MAX_STEPS_AFTER_FOOD = 200 # avoid loop\n",
    "\n",
    "\n",
    "    def grid_distance(self, pos1, pos2):\n",
    "        # calculate euclidean distance between 2 points\n",
    "        return np.linalg.norm(np.array(pos1, dtype=np.float32) - np.array(pos2, dtype=np.float32))\n",
    "\n",
    "    \n",
    "    def __init__(self, grid_size=20):\n",
    "        super(SnakeGame, self).__init__()\n",
    "\n",
    "        # steps init\n",
    "        self.stepnum = 0\n",
    "        self.last_food_step = 0\n",
    "\n",
    "        # grid init\n",
    "        self.grid_size = grid_size\n",
    "        self.grid = np.zeros((self.grid_size, self.grid_size), dtype=np.uint8) + self.EMPTY # EMPTY is zero so it doesn't matter (in case its not)\n",
    "        \n",
    "        # wall init\n",
    "        self.grid[0, :] = self.WALL # UP\n",
    "        self.grid[:, 0] = self.WALL # LEFT\n",
    "        self.grid[self.grid_size - 1, :] = self.WALL # DOWN\n",
    "        self.grid[:, self.grid_size - 1] = self.WALL # RIGHT\n",
    "\n",
    "        # snake init\n",
    "        # self.snake_coordinates = [ (1,1), (2,1) ]\n",
    "        self.snake_coord = [(4, 3), (4, 4)] # top left\n",
    "\n",
    "        for coord in self.snake_coord:\n",
    "            self.grid[coord] = self.SNAKE\n",
    "\n",
    "        # food init\n",
    "        self.grid[3, 3] = self.FOOD\n",
    "\n",
    "        # distance calculation\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        # save init setup\n",
    "        self.init_grid = self.grid.copy()\n",
    "        self.init_snake_coord = self.snake_coord.copy()\n",
    "\n",
    "        # action space\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "\n",
    "        # observation(state) space\n",
    "        self.observation_space = spaces.Dict(\n",
    "            spaces={\n",
    "                \"position\" : spaces.Box(low=0, high=(self.grid_size - 1), shape=(2,), dtype=np.int32),\n",
    "                \"direction\" : spaces.Box(low=-1, high=1, shape=(2,), dtype=np.int32),\n",
    "                \"grid\" : spaces.Box(low=0, high=3, shape=(self.grid_size * self.grid_size,), dtype=np.uint8)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        import random\n",
    "        # to init position\n",
    "        self.stepnum = 0\n",
    "        self.last_food_step = 0\n",
    "        self.grid = self.init_grid.copy()\n",
    "        self.snake_coord = self.init_snake_coord.copy()\n",
    "\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        obs = self._get_obs() # state space\n",
    "        info = {}\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        position = np.array(self.snake_coord[-1], dtype=np.int32)\n",
    "        direction = (np.array(self.snake_coord[-1]) - np.array(self.snake_coord[-2])).astype(np.int32)\n",
    "        grid = self.grid.flatten()\n",
    "\n",
    "        obs = {\n",
    "            \"position\" : position,\n",
    "            \"direction\" : direction,\n",
    "            \"grid\" : grid\n",
    "        }\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        direction = np.array(self.snake_coord[-1]) - np.array(self.snake_coord[-2])\n",
    "\n",
    "        if action == self.STRAIGHT:\n",
    "            step = direction # towards the direction the snake faces\n",
    "        elif action == self.RIGHT:\n",
    "            # rotation matrix\n",
    "            step = np.array( [direction[1], -direction[0]] )\n",
    "        elif action == self.LEFT:\n",
    "            step = np.array( [-direction[1], direction[0]] )\n",
    "        \n",
    "        # new head\n",
    "        new_coord = (np.array(self.snake_coord[-1]) + step).astype(np.int32)\n",
    "\n",
    "        if not (0 <= new_coord[0] < self.grid_size and 0 <= new_coord[1] < self.grid_size):\n",
    "            return self._get_obs(), self.REWARD_WALL_HIT, True, False, {}\n",
    "\n",
    "        self.snake_coord.append( (new_coord[0], new_coord[1]) )\n",
    "\n",
    "        new_pos = self.snake_coord[-1]\n",
    "        new_pos_type = self.grid[new_pos]\n",
    "        self.grid[new_pos] = self.SNAKE\n",
    "\n",
    "        done = False\n",
    "        reward = 0 # calculated later\n",
    "\n",
    "        if new_pos_type == self.FOOD:\n",
    "            reward += self.REWARD_PER_FOOD\n",
    "            self.last_food_step = self.stepnum\n",
    "\n",
    "            # new food\n",
    "            empty_tiles = np.argwhere(self.grid == self.EMPTY)\n",
    "\n",
    "            if len(empty_tiles):\n",
    "                new_food_pos = empty_tiles[np.random.randint(0, len(empty_tiles))]\n",
    "                self.grid[new_food_pos[0], new_food_pos[1]] = self.FOOD\n",
    "            else:\n",
    "                done = True\n",
    "            \n",
    "        else:\n",
    "            self.grid[self.snake_coord[0]] = self.EMPTY # empty the snake tail\n",
    "            self.snake_coord = self.snake_coord[1:]\n",
    "\n",
    "            if (new_pos_type == self.WALL) or (new_pos_type == self.SNAKE):\n",
    "                done = True\n",
    "                reward += self.REWARD_WALL_HIT\n",
    "        \n",
    "        head_dist_to_food_prev = self.head_dist_to_food\n",
    "        self.head_dist_to_food = self.grid_distance(\n",
    "            self.snake_coord[-1],\n",
    "            np.argwhere(self.grid == self.FOOD)[0]\n",
    "        )\n",
    "\n",
    "        # reward for distance between snake <-> food\n",
    "        if head_dist_to_food_prev > self.head_dist_to_food:\n",
    "            reward += self.REWARD_PER_STEP_TOWARDS_FOOD\n",
    "        elif head_dist_to_food_prev < self.head_dist_to_food:\n",
    "            reward -= self.REWARD_PER_STEP_TOWARDS_FOOD * 2\n",
    "        \n",
    "        # max steps since no food\n",
    "        if ((self.stepnum - self.last_food_step) > self.MAX_STEPS_AFTER_FOOD):\n",
    "            done = True\n",
    "        \n",
    "        self.stepnum += 1\n",
    "\n",
    "        # print(f\"Step: {self.stepnum}, Action: {action}, Reward: {reward}, Done: {done}\")\n",
    "        # print(f\"Snake Head: {self.snake_coord[-1]}, Distance to Food: {self.head_dist_to_food}\")\n",
    "        # print(f\"New Position Type: {new_pos_type}\")\n",
    "\n",
    "        # return observation, reward, done, truncated, info\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "\n",
    "    def snake_plot(self, plot_inline=False):\n",
    "        wall_idx = (self.grid == self.WALL)\n",
    "        snake_idx = (self.grid == self.SNAKE)\n",
    "        food_idx = (self.grid == self.FOOD)\n",
    "\n",
    "        # colour array for plot\n",
    "        colour_arr = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.uint8) + 255 # default to white\n",
    "        colour_arr[wall_idx, :] = np.array([0, 0, 0])\n",
    "        colour_arr[snake_idx, :] = np.array([255, 196, 0])\n",
    "        colour_arr[food_idx, :] = np.array([30, 47, 135])\n",
    "\n",
    "        return colour_arr\n",
    "    \n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        if mode == 'console':\n",
    "            print(self.grid)\n",
    "        elif mode == 'rgb_array':\n",
    "            return self.snake_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env check\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = SnakeGame()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os\n",
    "\n",
    "# log\n",
    "log_dir = \"../log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# env\n",
    "env = SnakeGame()\n",
    "\n",
    "# wrap env with monitor\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# cb fn -> periodically evaluate the model and save the best version\n",
    "eval_cb = EvalCallback(env, best_model_save_path='../best_model',\n",
    "                       log_path='../log',\n",
    "                       eval_freq=5000,\n",
    "                       deterministic=False,\n",
    "                       render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=5000, episode_reward=-177.00 +/- 52.21\n",
      "Episode length: 35.40 +/- 23.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-78.00 +/- 76.39\n",
      "Episode length: 48.00 +/- 49.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-244.00 +/- 223.48\n",
      "Episode length: 68.40 +/- 49.89\n",
      "Eval num_timesteps=20000, episode_reward=-198.00 +/- 156.93\n",
      "Episode length: 39.80 +/- 22.14\n",
      "Eval num_timesteps=25000, episode_reward=-134.00 +/- 143.40\n",
      "Episode length: 48.60 +/- 66.03\n",
      "Eval num_timesteps=30000, episode_reward=-40.00 +/- 43.82\n",
      "Episode length: 12.20 +/- 8.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-3.00 +/- 83.22\n",
      "Episode length: 21.00 +/- 22.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=12.00 +/- 6.00\n",
      "Episode length: 11.00 +/- 3.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-5.00 +/- 19.49\n",
      "Episode length: 17.80 +/- 16.70\n",
      "Eval num_timesteps=50000, episode_reward=-49.00 +/- 65.76\n",
      "Episode length: 13.40 +/- 4.32\n",
      "Eval num_timesteps=55000, episode_reward=32.00 +/- 51.83\n",
      "Episode length: 17.60 +/- 12.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 17.32\n",
      "Episode length: 13.40 +/- 10.46\n",
      "Eval num_timesteps=65000, episode_reward=-31.00 +/- 68.07\n",
      "Episode length: 18.60 +/- 18.87\n",
      "Eval num_timesteps=70000, episode_reward=-17.00 +/- 37.50\n",
      "Episode length: 9.20 +/- 3.37\n",
      "Eval num_timesteps=75000, episode_reward=-9.00 +/- 38.39\n",
      "Episode length: 13.80 +/- 8.77\n",
      "Eval num_timesteps=80000, episode_reward=4.00 +/- 22.00\n",
      "Episode length: 10.60 +/- 3.38\n",
      "Eval num_timesteps=85000, episode_reward=20.00 +/- 14.49\n",
      "Episode length: 9.00 +/- 3.79\n",
      "Eval num_timesteps=90000, episode_reward=4.00 +/- 33.38\n",
      "Episode length: 15.40 +/- 15.97\n",
      "Eval num_timesteps=95000, episode_reward=38.00 +/- 46.32\n",
      "Episode length: 9.40 +/- 6.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=12.00 +/- 12.08\n",
      "Episode length: 9.20 +/- 2.99\n",
      "Eval num_timesteps=105000, episode_reward=-7.00 +/- 32.80\n",
      "Episode length: 19.80 +/- 16.45\n",
      "Eval num_timesteps=110000, episode_reward=-61.00 +/- 115.26\n",
      "Episode length: 29.40 +/- 33.12\n",
      "Eval num_timesteps=115000, episode_reward=13.00 +/- 9.27\n",
      "Episode length: 12.40 +/- 12.82\n",
      "Eval num_timesteps=120000, episode_reward=42.00 +/- 44.68\n",
      "Episode length: 14.00 +/- 12.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=-4.00 +/- 26.91\n",
      "Episode length: 22.20 +/- 22.21\n",
      "Eval num_timesteps=130000, episode_reward=-3.00 +/- 16.61\n",
      "Episode length: 15.80 +/- 17.14\n",
      "Eval num_timesteps=135000, episode_reward=11.00 +/- 15.94\n",
      "Episode length: 11.40 +/- 6.47\n",
      "Eval num_timesteps=140000, episode_reward=-41.00 +/- 88.74\n",
      "Episode length: 26.20 +/- 25.84\n",
      "Eval num_timesteps=145000, episode_reward=12.00 +/- 59.30\n",
      "Episode length: 16.00 +/- 9.19\n",
      "Eval num_timesteps=150000, episode_reward=-15.00 +/- 37.95\n",
      "Episode length: 17.00 +/- 12.73\n",
      "Eval num_timesteps=155000, episode_reward=14.00 +/- 63.98\n",
      "Episode length: 20.60 +/- 22.42\n",
      "Eval num_timesteps=160000, episode_reward=-2.00 +/- 23.15\n",
      "Episode length: 8.20 +/- 2.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x14903cce0>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "# Proximal Policy Optimization\n",
    "\n",
    "# PPO hyperparam\n",
    "PPO_model_args = {\n",
    "    \"learning_rate\" : 0.002,\n",
    "    \"gamma\" : 0.99, # discount factor for further rewards [0, 1]\n",
    "    \"verbose\" : 0, # 1 -> more info on training steps\n",
    "    \"seed\" : 523,\n",
    "    \"ent_coef\" : 0.2, # entropy coef -> encourage exploration\n",
    "    \"clip_range\" : 0.2 # limits p of action difference\n",
    "}\n",
    "\n",
    "# Multi Input Policy since we have 1+ states as an 'input'\n",
    "model = PPO('MultiInputPolicy', env, **PPO_model_args)\n",
    "\n",
    "if os.path.exists(\"../best_model/best_model.zip\"):\n",
    "    model.set_parameters(\"../best_model/best_model.zip\")\n",
    "\n",
    "model.learn(6000000, callback=eval_cb)\n",
    "# model.learn(160000, callback=eval_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 \n",
      "Action: 0 \n",
      "Total Reward: 5\n",
      "Step 2 \n",
      "Action: 0 \n",
      "Total Reward: 95\n",
      "Step 3 \n",
      "Action: 2 \n",
      "Total Reward: 85\n",
      "Step 4 \n",
      "Action: 1 \n",
      "Total Reward: 75\n",
      "Step 5 \n",
      "Action: 2 \n",
      "Total Reward: 80\n",
      "Step 6 \n",
      "Action: 1 \n",
      "Total Reward: 85\n",
      "Step 7 \n",
      "Action: 2 \n",
      "Total Reward: 90\n",
      "Step 8 \n",
      "Action: 1 \n",
      "Total Reward: 95\n",
      "Step 9 \n",
      "Action: 1 \n",
      "Total Reward: 100\n",
      "Step 10 \n",
      "Action: 1 \n",
      "Total Reward: 90\n",
      "Step 11 \n",
      "Action: 1 \n",
      "Total Reward: 80\n",
      "Step 12 \n",
      "Action: 1 \n",
      "Total Reward: 70\n",
      "Step 13 \n",
      "Action: 2 \n",
      "Total Reward: 60\n",
      "Step 14 \n",
      "Action: 1 \n",
      "Total Reward: 50\n",
      "Step 15 \n",
      "Action: 0 \n",
      "Total Reward: 40\n",
      "Step 16 \n",
      "Action: 0 \n",
      "Total Reward: 45\n",
      "Step 17 \n",
      "Action: 2 \n",
      "Total Reward: 35\n",
      "Step 18 \n",
      "Action: 2 \n",
      "Total Reward: 25\n",
      "Step 19 \n",
      "Action: 2 \n",
      "Total Reward: 30\n",
      "Step 20 \n",
      "Action: 1 \n",
      "Total Reward: 35\n",
      "Step 21 \n",
      "Action: 0 \n",
      "Total Reward: 25\n",
      "Step 22 \n",
      "Action: 1 \n",
      "Total Reward: 15\n",
      "Step 23 \n",
      "Action: 1 \n",
      "Total Reward: -45\n",
      "Game Over! Total Reward: -45\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "\n",
    "# test\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# for img gif\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "frames = []\n",
    "fps = 18\n",
    "\n",
    "n_steps = 1000000\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # preprocess the obs to match the model's input format\n",
    "    action, _ = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(f\"Step {step + 1} \\nAction: {action} \\nTotal Reward: {total_reward}\")\n",
    "    \n",
    "    frames.append([ax.imshow(env.unwrapped.render(mode='rgb_array'), animated=True)])\n",
    "\n",
    "    if done:\n",
    "        print(f\"Game Over! Total Reward: {total_reward}\")\n",
    "        break\n",
    "\n",
    "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
    "\n",
    "anim = animation.ArtistAnimation(fig, frames, interval=int(1000/fps), blit=True, repeat_delay=1000)\n",
    "anim.save(\"../snake_game.gif\", dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
